{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n",
    "\n",
    "These are two examples of topic classification, categorizing a text document into one of a predefined set of topics. In many topic classification problems, this categorization is based primarily on keywords in the text.\n",
    "\n",
    "![](img/TextClassificationExample.png)\n",
    "<p style=\"text-align:center\">**Figure 1: Topic classification is used to flag incoming spam emails, which are filtered into a spam folder.**</p>\n",
    "\n",
    "Another common type of text classification is **sentiment analysis**, whose goal is to identify the polarity of text content: the type of opinion it expresses. This can take the form of a binary like/dislike rating, or a more granular set of options, such as a star rating from 1 to 5. Examples of sentiment analysis include analyzing Twitter posts to determine if people liked the Black Panther movie, or extrapolating the general public’s opinion of a new brand of Nike shoes from Walmart reviews.\n",
    "\n",
    "This guide will teach you some key machine learning best practices for solving text classification problems. Here’s what you’ll learn:\n",
    "* The high-level, end-to-end workflow for solving text classification problems using machine learning\n",
    "* How to choose the right model for your text classification problem\n",
    "* How to implement your model of choice using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Workflow\n",
    "Here’s a high-level overview of the workflow used to solve machine learning problems:\n",
    "\n",
    "* Step 1: Gather Data\n",
    "* Step 2: Explore Your Data\n",
    "* Step 2.5: Choose a Model*\n",
    "* Step 3: Prepare Your Data\n",
    "* Step 4: Build, Train, and Evaluate Your Model\n",
    "* Step 5: Tune Hyperparameters\n",
    "* Step 6: Deploy Your Model\n",
    "![](img/Workflow.png) \n",
    "<p style=\"text-align:center\">**Figure 2: Workflow for solving machine learning problems**</p>\n",
    "\n",
    "<table align=\"left\"><tr><td bgcolor='#E1F5FE'>\n",
    "<p>**Note:** “Choose a model” is not a formal step of the traditional machine learning workflow; however, selecting an appropriate model</p>\n",
    "<p>for your problem is a critical task that clarifies and simplifies the work in the steps that follow.</p>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections explain each step in detail, and how to implement them for text data.\n",
    "# Step 1: Gather Data\n",
    "Gathering data is the most important step in solving any supervised machine learning problem. Your text classifier can only be as good as the dataset it is built from.\n",
    "\n",
    "If you don’t have a specific problem you want to solve and are just interested in exploring text classification in general, there are plenty of open source datasets available. You can find links to some of them in our [GitHub repo](https://github.com/google/eng-edu/blob/master/ml/guides/text_classification/load_data.py). On the other hand, if you are tackling a specific problem, you will need to collect the necessary data. Many organizations provide public APIs for accessing their data—for example, the [Twitter API](https://dev.twitter.com/rest/public) or the [NY Times API](http://developer.nytimes.com/). You may be able to leverage these for the problem you are trying to solve.\n",
    "\n",
    "Here are some important things to remember when collecting data:\n",
    "\n",
    "* If you are using a public API, understand the _limitations_ of the API before using them. For example, some APIs set a limit on the rate at which you can make queries.  \n",
    "\n",
    "\n",
    "* The more training examples (referred to as _samples_ in the rest of this guide) you have, the better. This will help your model generalize better.  \n",
    "\n",
    "\n",
    "* Make sure the number of samples for every _class_ or topic is not overly _imbalanced_. That is, you should have comparable number of samples in each class.  \n",
    "\n",
    "\n",
    "* Make sure that your samples adequately cover the _space of possible inputs_, not only the common cases.  \n",
    "\n",
    "Throughout this guide, we will use the [Internet Movie Database (IMDb) movie reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/) to illustrate the workflow. This dataset contains movie reviews posted by people on the IMDb website, as well as the corresponding labels (“positive” or “negative”) indicating whether the reviewer liked the movie or not. This is a classic example of a sentiment analysis problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore Your Data\n",
    "Building and training a model is only one part of the workflow. Understanding the characteristics of your data beforehand will enable you to build a better model. This could simply mean obtaining a higher accuracy. It could also mean requiring less data for training, or fewer computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "First up, let’s load the dataset into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname), 'r', encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname), 'r', encoding='utf-8') as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Data\n",
    "After loading the data, it’s good practice to **run some checks** on it: pick a few samples and manually check if they are consistent with your expectations. For example, print a few random samples to see if the sentiment label corresponds to the sentiment of the review. Here is a review we picked at random from the IMDb dataset: _“Ten minutes worth of story stretched out into the better part of two hours. When nothing of any significance had happened at the halfway point I should have left.”_ The expected sentiment (negative) matches the sample’s label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Key Metrics\n",
    "Once you’ve verified the data, collect the following important metrics that can help characterize your text classification problem:\n",
    "\n",
    "1. **Number of samples**: Total number of examples you have in the data.\n",
    "\n",
    "2. **Number of classes**: Total number of topics or categories in the data.\n",
    "\n",
    "3. **Number of samples per class**: Number of samples per class (topic/category). In a balanced dataset, all classes will have a similar number of samples; in an imbalanced dataset, the number of samples in each class will vary widely.\n",
    "\n",
    "4. **Number of words per sample**: Median number of words in one sample.\n",
    "\n",
    "5. **Frequency distribution of words**: Distribution showing the frequency (number of occurrences) of each word in the dataset.\n",
    "\n",
    "6. **Distribution of sample length**: Distribution showing the number of words per sample in the dataset.\n",
    "\n",
    "Let’s see what the values for these metrics are for the IMDb reviews dataset (See Figures 3 and 4 for plots of the word-frequency and sample-length distributions).\n",
    "\n",
    "<p align=\"left\">Metric name|Metric value</p>\n",
    ":---|:--:\n",
    "<p align=\"left\">Number of samples</p>|<center>25000</center>\n",
    "<p align=\"left\">Number of classes</p>|<center>2</center>\n",
    "<p align=\"left\">Number of samples per class</p>|<center>12500</center>\n",
    "<p align=\"left\">Number of words per sample</p>|<center>174</center>\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\">**Table 1: IMDb reviews dataset metrics**</p>\n",
    "\n",
    "[explore_data.py](https://github.com/google/eng-edu/blob/master/ml/guides/text_classification/explore_data.py) contains functions to calculate and analyse these metrics. Here are a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Returns the median number of words per sample given corpus.\n",
    "\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "\n",
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'x:/E/CodeBases/AI/NLP/Corpus'\n",
    "(train_texts, _), (test_texts, _) = load_imdb_sentiment_analysis_dataset(data_path)\n",
    "sample_texts = train_texts + test_texts\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28a007c2be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequency_distribution_of_ngrams(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28a7f23cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2.5: Choose a Model\n",
    "At this point, we have assembled our dataset and gained insights into the key characteristics of our data. Next, based on the metrics we gathered in [Step 2](), we should think about which classification model we should use. This means/ asking questions such as, “How do we present the text data to an algorithm that expects numeric input?” (this is called data preprocessing and vectorization), “What type of model should we use?”, “What configuration parameters should we use for our model?”, etc.\n",
    "\n",
    "Thanks to decades of research, we have access to a large array of data preprocessing and model configuration options. However, the availability of a very large array of viable options to choose from greatly increases the complexity and the scope of the particular problem at hand. Given that the best options might not be obvious, a naive solution would be to try every possible option exhaustively, pruning some choices through intuition. However, that would be tremendously expensive.\n",
    "\n",
    "In this guide, we attempt to significantly simplify the process of selecting a text classification model. For a given dataset, our goal is to find the algorithm that achieves close to maximum accuracy while minimizing computation time required for training. We ran a large number (~450K) of experiments across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures. This helped us identify dataset parameters that influence optimal choices.\n",
    "\n",
    "The model selection algorithm and flowchart below are a summary of our experimentation. Don’t worry if you don’t understand all the terms used in them yet; the following sections of this guide will explain them in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for Data Preparation and Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. Calculate the number of samples/number of words per sample ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a\n",
    "simple multi-layer perceptron (MLP) model to classify them (left branch in the\n",
    "flowchart below):\n",
    "  a. Split the samples into word n-grams; convert the n-grams into vectors.\n",
    "  b. Score the importance of the vectors and then select the top 20K using the scores.\n",
    "  c. Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a\n",
    "   sepCNN model to classify them (right branch in the flowchart below):\n",
    "  a. Split the samples into words; select the top 20K words based on their frequency.\n",
    "  b. Convert the samples into word sequence vectors.\n",
    "  c. If the original number of samples/number of words per sample ratio is less\n",
    "     than 15K, using a fine-tuned pre-trained embedding with the sepCNN\n",
    "     model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find\n",
    "   the best model configuration for the dataset.\n",
    "```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the flowchart below, the yellow boxes indicate data and model preparation processes. Grey boxes and green boxes indicate choices we considered for each process. Green boxes indicate our recommended choice for each process.\n",
    "\n",
    "You can use this flowchart as a starting point to construct your first experiment, as it will give you good accuracy at low computation costs. You can then continue to improve on your initial model over the subsequent iterations.\n",
    "\n",
    "![](img/TextClassificationFlowchart.png)\n",
    "<p style=\"text-align:center\">**Figure 5: Text classification flowchart**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flowchart answers two key questions:\n",
    "\n",
    "1. Which learning algorithm or model should we use?\n",
    "\n",
    "2. How should we prepare the data to efficiently learn the relationship between text and label?\n",
    "\n",
    "The answer to the second question depends on the answer to the first question; the way we preprocess data to be fed into a model will depend on what model we choose. Models can be broadly classified into two categories: those that use word ordering information (sequence models), and ones that just see text as “bags” (sets) of words (n-gram models). Types of sequence models include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their variations. Types of n-gram models include [logistic regression](https://wikipedia.org/wiki/Logistic_regression), [simple multi- layer perceptrons](https://wikipedia.org/wiki/Multilayer_perceptron) (MLPs, or fully-connected neural networks), [gradient boosted trees](https://wikipedia.org/wiki/Gradient_boosting) and [support vector machines](https://wikipedia.org/wiki/Support_vector_machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From our experiments, we have observed that the ratio of “number of samples” (S) to “number of words per sample” (W) correlates with which model performs well.**\n",
    "\n",
    "When the value for this ratio is small (<1500), small multi-layer perceptrons that take n-grams as input (which we'll call **Option A**) perform better or at least as well as sequence models. MLPs are simple to define and understand, and they take much less compute time than sequence models. When the value for this ratio is large (>= 1500), use a sequence model (**Option B**). In the steps that follow, you can skip to the relevant subsections (labeled **A** or **B**) for the model type you chose based on the samples/words-per-sample ratio.\n",
    "\n",
    "In the case of our IMDb review dataset, the samples/words-per-sample ratio is ~144. This means that we will create a MLP model.\n",
    "\n",
    "<table align=\"left\"><tr><td bgcolor='#E1F5FE'>\n",
    "<p>**Note**: When using the above flowchart, keep in mind that it may not necessarily lead you to the most optimal results for your problem, for several reasons: </p>\n",
    "<p>- Your goal may be different. We optimized for the best accuracy that could be achieved in the shortest possible compute time. An alternate flow may produce a better result, say, when optimizing for [area under the curve (AUC)](https://developers.google.com/machine-learning/glossary#AUC). </p>\n",
    "<p>- We picked typical and common algorithm choices. As the field continues to evolve, new cutting-edge algorithms and enhancements may be relevant to your data and may perform better. </p>\n",
    "<p>- While we used several datasets to derive and validate the flowchart, there may be specific characteristics to your dataset that favor using an alternate flow.</p>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare Your Data\n",
    "Before our data can be fed to a model, it needs to be transformed to a format the model can understand.\n",
    "\n",
    "First, the data samples that we have gathered may be in a specific order. We do not want any information associated with the ordering of samples to influence the relationship between texts and labels. For example, if a dataset is sorted by class and is then split into training/validation sets, these sets will not be representative of the overall distribution of data.\n",
    "\n",
    "A simple best practice to ensure the model is not affected by data order is to always shuffle the data before doing anything else. If your data is already split into training and validation sets, make sure to transform your validation data the same way you transform your training data. If you don’t already have separate training and validation sets, you can split the samples after shuffling; it’s typical to use 80% of the samples for training and 20% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "\n",
    "1. _**Tokenization**_: Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data).\n",
    "\n",
    "2. _**Vectorization**_: Define a good numerical measure to characterize these texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how to perform these two steps for both n-gram vectors and sequence vectors, as well as how to optimize the vector representations using feature selection and normalization techniques.\n",
    "## N-gram vectors [Option A]\n",
    "In the subsequent paragraphs, we will see how to do tokenization and vectorization for n-gram models. We will also cover how we can optimize the n- gram representation using feature selection and normalization techniques.\n",
    "\n",
    "In an n-gram vector, text is represented as a collection of unique n-grams: groups of n adjacent tokens (typically, words). Consider the text **The mouse ran up the clock**. Here, the word unigrams (n = 1) are **['the', 'mouse', 'ran', 'up', 'clock']**, the word bigrams (n = 2) are **['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock']**, and so on.\n",
    "\n",
    "### Tokenization\n",
    "We have found that tokenizing into word unigrams + bigrams provides good accuracy while taking less compute time.\n",
    "\n",
    "### Vectorization\n",
    "Once we have split our text samples into n-grams, we need to turn these n-grams into numerical vectors that our machine learning models can process. The example below shows the indexes assigned to the unigrams and bigrams generated for two texts.\n",
    "\n",
    "<table align=\"left\"><tr><td bgcolor='#F0F0F0'>\n",
    "    <p>Texts: 'The mouse ran up the clock' and 'The mouse ran down'</p>\n",
    "    <p>Index assigned for every token: </p>\n",
    "    <p>{'the': 7, 'mouse': 2, 'ran': 4, 'up': 10,\n",
    "'clock': 0, 'the mouse': 9, 'mouse ran': 3, 'ran up': 6, 'up the': 11, 'the\n",
    "        clock': 8, 'down': 1, 'ran down': 5}</p>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once indexes are assigned to the n-grams, we typically vectorize using one of the following options.\n",
    "\n",
    "**One-hot encoding**: Every sample text is represented as a vector indicating the presence or absence of a token in the text.\n",
    "<table align=\"left\"><tr><td bgcolor='#F0F0F0'>\n",
    "'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count encoding**: Every sample text is represented as a vector indicating the count of a token in the text. Note that the element corresponding to the unigram 'the' (bolded below) now is represented as 2 because the word “the” appears twice in the text.\n",
    "<table align=\"left\"><tr><td bgcolor='#F0F0F0'>\n",
    "'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1]\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Tf-idf encoding](https://wikipedia.org/wiki/Tf%E2%80%93idf)**: The problem with the above two approaches is that common words that occur in similar frequencies in all documents (i.e., words that are not particularly unique to the text samples in the dataset) are not penalized. For example, words like “a” will occur very frequently in all texts. So a higher token count for “the” than for other more meaningful words is not very useful.\n",
    "<table align=\"left\"><tr><td bgcolor='#F0F0F0'>\n",
    "'The mouse ran up the clock' = [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33,\n",
    "0.23, 0.33, 0.33] (See [Scikit-learn TdidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html))\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other vector representations, but the above three are the most commonly used.\n",
    "\n",
    "We observed that tf-idf encoding is marginally better than the other two in terms of accuracy (on average: 0.25-15% higher), and recommend using this method for vectorizing n-grams. However, keep in mind that it occupies more memory (as it uses floating-point representation) and takes more time to compute, especially for large datasets (can take twice as long in some cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "When we convert all of the texts in a dataset into word uni+bigram tokens, we may end up with tens of thousands of tokens. Not all of these tokens/features contribute to label prediction. So we can drop certain tokens, for instance those that occur extremely rarely across the dataset. We can also measure feature importance (how much each token contributes to label predictions), and only include the most informative tokens.\n",
    "\n",
    "There are many statistical functions that take features and the corresponding labels and output the feature importance score. Two commonly used functions are f_classif and chi2. Our experiments show that both of these functions perform equally well.\n",
    "\n",
    "More importantly, we saw that accuracy peaks at around 20,000 features for many datasets (See Figure 6). Adding more features over this threshold contributes very little and sometimes even leads to overfitting and degrades performance.\n",
    "<table align=\"left\"><tr><td bgcolor='#E1F5FE'>\n",
    "<p>We used exclusively English texts in our tests here. The ideal number of features may vary by languages; this could be</p><p> explored in follow-up analyses.</p>\n",
    "</td></tr></table>\n",
    "![](img/TopKvsAccuracy.svg)\n",
    "<p style=\"text-align:center\">**Figure 6: Top K features vs Accuracy**. Across datasets, accuracy plateaus at around top 20K features.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization \n",
    "Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step.\n",
    "\n",
    "The following code puts together all of the above steps:\n",
    "\n",
    "* Tokenize text samples into word uni+bigrams,\n",
    "* Vectorize using tf-idf encoding,\n",
    "* Select only the top 20,000 features from the vector of tokens by discarding tokens that appear fewer than 2 times and using f_classif to calculate feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With n-gram vector representation, we discard a lot of information about word order and grammar (at best, we can maintain some partial ordering information when n > 1). This is called a bag-of-words approach. This representation is used in conjunction with models that don’t take ordering into account, such as logistic regression, multi-layer perceptrons, gradient boosting machines, support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Vectors [Option B]\n",
    "In the subsequent paragraphs, we will see how to do tokenization and vectorization for sequence models. We will also cover how we can optimize the sequence representation using feature selection and normalization techniques.\n",
    "\n",
    "For some text samples, word order is critical to the text’s meaning. For example, the sentences, “I used to hate my commute. My new bike changed that completely” can be understood only when read in order. Models such as CNNs/RNNs can infer meaning from the order of words in a sample. For these models, we represent the text as a sequence of tokens, preserving order.\n",
    "\n",
    "### Tokenization\n",
    "Text can be represented as either a sequence of characters, or a sequence of words. We have found that using word-level representation provides better performance than character tokens. This is also the general norm that is followed by industry. Using character tokens makes sense only if texts have lots of typos, which isn’t normally the case.\n",
    "\n",
    "### Vectorization\n",
    "Once we have converted our text samples into sequences of words, we need to turn these sequences into numerical vectors. The example below shows the indexes assigned to the unigrams generated for two texts, and then the sequence of token indexes to which the first text is converted.\n",
    "```\n",
    "Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
    "Index assigned for every token: {'clock': 5, 'ran': 3, 'up': 4, 'down': 6, 'the': 1, 'mouse': 2}.\n",
    "NOTE: 'the' occurs most frequently, so the index value of 1 is assigned to it.\n",
    "Some libraries reserve index 0 for unknown tokens, as is the case here.\n",
    "Sequence of token indexes: 'The mouse ran up the clock' = [1, 2, 3, 4, 1, 5]\n",
    "```\n",
    "There are two options available to vectorize the token sequences:  \n",
    "\n",
    "_**One-hot encoding**_: Sequences are represented using word vectors in n- dimensional space where n = size of vocabulary. This representation works great when we are tokenizing as characters, and the vocabulary is therefore small. When we are tokenizing as words, the vocabulary will usually have tens of thousands of tokens, making the one-hot vectors very sparse and inefficient. Example:\n",
    "```\n",
    "'The mouse ran up the clock' = [\n",
    "  [0, 1, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 1, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 1, 0, 0],\n",
    "  [0, 1, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 1, 0]\n",
    "]\n",
    "```\n",
    "\n",
    "_**Word embeddings**_: Words have meaning(s) associated with them. As a result, we can represent word tokens in a dense vector space (~few hundred real numbers), where the location and distance between words indicates how similar they are semantically (See [Figure 7]()). This representation is called [word embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=mlcc&utm_term=embeddings).\n",
    "![](img/WordEmbeddings.png)\n",
    "<p style=\"text-align:center\">**Figure 7: Word embeddings**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence models often have such an embedding layer as their first layer. This layer learns to turn word index sequences into word embedding vectors during the training process, such that each word index gets mapped to a dense vector of real values representing that word’s location in semantic space (See [Figure 8]()).\n",
    "![](img/EmbeddingLayer.png)\n",
    "<p style=\"text-align:center\">**Figure 8: Embedding layer**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Not all words in our data contribute to label predictions. We can optimize our learning process by discarding rare or irrelevant words from our vocabulary. In fact, we observe that using the most frequent 20,000 features is generally sufficient. This holds true for n-gram models as well (See Figure 6).\n",
    "\n",
    "Let’s put all of the above steps in sequence vectorization together. The following code performs these tasks:\n",
    "\n",
    "* Tokenizes the texts into words\n",
    "* Creates a vocabulary using the top 20,000 tokens\n",
    "* Converts the tokens into sequence vectors\n",
    "* Pads the sequences to a fixed sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label vectorization\n",
    "We saw how to convert sample text data into numerical vectors. A similar process must be applied to the labels. We can simply convert labels into values in range [0, num_classes - 1]. For example, if there are 3 classes we can just use values 0, 1 and 2 to represent them. Internally, the network will use one-hot vectors to represent these values (to avoid inferring an incorrect relationship between labels). This representation depends on the loss function and the last- layer activation function we use in our neural network. We will learn more about these in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Build, Train, and Evaluate Your Model\n",
    "In this section, we will work towards building, training and evaluating our model. In [Step 3](), we chose to use either an n-gram model or sequence model, using our S/W ratio. Now, it’s time to write our classification algorithm and train it. We will use [TensorFlow](https://www.tensorflow.org/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=tensorflow&utm_term=tensorflow) with the [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=tensorflow&utm_term=keras) API for this.\n",
    "\n",
    "Building machine learning models with Keras is all about assembling together layers, data-processing building blocks, much like we would assemble Lego bricks. These layers allow us to specify the sequence of transformations we want to perform on our input. As our learning algorithm takes in a single text input and outputs a single classification, we can create a linear stack of layers using the [Sequential model](https://keras.io/getting-started/sequential-model-guide/) API.\n",
    "![](img/LinearStackOfLayers.png)\n",
    "<p style=\"text-align:center\">**Figure 9: Linear stack of layers**</p>  \n",
    "\n",
    "The input layer and the intermediate layers will be constructed differently, depending on whether we’re building an n-gram or a sequence model. But irrespective of model type, the last layer will be the same for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Last Layer\n",
    "When we have only 2 classes (binary classification), our model should output a single probability score. For instance, outputting 0.2 for a given input sample means “20% confidence that this sample is in class 0, 80% that it is in class 1.” To output such a probability score, the [activation function](https://developers.google.com/machine-learning/crash-course/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=activation-function#activation_function) of the last layer should be a [sigmoid function](https://developers.google.com/machine-learning/crash-course/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=sigmoid-function#sigmoid_function), and the [loss function](https://wikipedia.org/wiki/Loss_function) used to train the model should be [binary cross-entropy](https://wikipedia.org/wiki/Cross_entropy) (See [**Figure 10**](), left).\n",
    "\n",
    "When there are more than 2 classes (multi-class classification), our model should output one probability score per class. The sum of these scores should be 1. For instance, outputting {0: 0.2, 1: 0.7, 2: 0.1} means “20% confidence that this sample is in class 0, 70% that it is in class 1, and 10% that it is in class 2.” To output these scores, the activation function of the last layer should be softmax, and the loss function used to train the model should be categorical cross-entropy. (See [**Figure 10**](), right).\n",
    "![](img/LastLayer.png)\n",
    "<p style=\"text-align:center\">**Figure 10: Last layer**</p>   \n",
    "\n",
    "\n",
    "The following code defines a function that takes the number of classes as input, and outputs the appropriate number of layer units (1 unit for binary classification; otherwise 1 unit for each class) and the appropriate activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two sections walk through the creation of the remaining model layers for n-gram models and sequence models.\n",
    "\n",
    "When the S/W ratio is small, we’ve found that n-gram models perform better than sequence models. Sequence models are better when there are a large number of small, dense vectors. This is because embedding relationships are learned in dense space, and this happens best over many samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build n-gram model [Option A]\n",
    "We refer to models that process the tokens independently (not taking into account word order) as n-gram models. Simple multi-layer perceptrons (including [logistic regression](https://developers.google.com/machine-learning/crash-course/logistic-regression/video-lecture?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=mlcc&utm_term=logistic-regression)), [gradient boosting machines](https://wikipedia.org/wiki/Gradient_boosting) and [support vector machines](https://wikipedia.org/wiki/Support_vector_machine) models all fall under this category; they cannot leverage any information about text ordering.\n",
    "\n",
    "We compared the performance of some of the n-gram models mentioned above and observed that **multi-layer perceptrons (MLPs) typically perform better** than other options. MLPs are simple to define and understand, provide good accuracy, and require relatively little computation.\n",
    "\n",
    "The following code defines a two-layer MLP model in tf.keras, adding a couple of [Dropout layers for regularization](https://developers.google.com/machine-learning/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=dropout-regularization#dropout_regularization) (to prevent [overfitting](https://developers.google.com/machine-learning/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=overfitting#overfitting) to training samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sequence model [Option B]\n",
    "We refer to models that can learn from the adjacency of tokens as sequence models. This includes CNN and RNN classes of models. Data is pre-processed as sequence vectors for these models.\n",
    "\n",
    "Sequence models generally have a larger number of parameters to learn. The first layer in these models is an embedding layer, which learns the relationship between the words in a dense vector space. Learning word relationships works best over many samples.\n",
    "\n",
    "Words in a given dataset are most likely not unique to that dataset. We can thus learn the relationship between the words in our dataset using other dataset(s). To do so, we can transfer an embedding learned from another dataset into our embedding layer. These embeddings are referred to as **pre-trained embeddings**. Using a pre-trained embedding gives the model a head start in the learning process.\n",
    "\n",
    "There are pre-trained embeddings available that have been trained using large corpora, such as [GloVe](https://nlp.stanford.edu/projects/glove/). GloVe has been trained on multiple corpora (primarily Wikipedia). We tested training our sequence models using a version of GloVe embeddings and observed that if we froze the weights of the pre-trained embeddings and trained just the rest of the network, the models did not perform well. This could be because the context in which the embedding layer was trained might have been different from the context in which we were using it.\n",
    "\n",
    "GloVe embeddings trained on Wikipedia data may not align with the language patterns in our IMDb dataset. The relationships inferred may need some updating—i.e., the embedding weights may need contextual tuning. We do this in two stages:\n",
    "\n",
    "1. In the first run, with the embedding layer weights frozen, we allow the rest of the network to learn. At the end of this run, the model weights reach a state that is much better than their uninitialized values. For the second run, we allow the embedding layer to also learn, making fine adjustments to all weights in the network. We refer to this process as using a fine-tuned embedding.\n",
    "\n",
    "2. Fine-tuned embeddings yield better accuracy. However, this comes at the expense of increased compute power required to train the network. Given a sufficient number of samples, we could do just as well learning an embedding from scratch. We observed that for S/W > 15K, starting from scratch effectively yields about the same accuracy as using **fine-tuned embedding**.\n",
    "\n",
    "We compared different sequence models such as CNN, sepCNN ([Depthwise Separable Convolutional Network](https://arxiv.org/abs/1610.02357)), RNN (LSTM & GRU), CNN-RNN, and stacked RNN, varying the model architectures. We found that sepCNNs, a convolutional network variant that is often more data-efficient and compute-efficient, perform better than the other models.  \n",
    "\n",
    "<table align=\"left\"><tr><td bgcolor='#E1F5FE'>\n",
    "<p>**Note**: RNNs are relevant only to a small subset of use-cases. We did not try models like QRNN or RNNs with Attention, as</p>\n",
    "<p>their accuracy improvements would be offset by higher computational costs.</p>\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code constructs a four-layer sepCNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Model\n",
    "Now that we have constructed the model architecture, we need to train the model. Training involves making a prediction based on the current state of the model, calculating how incorrect the prediction is, and updating the weights or parameters of the network to minimize this error and make the model predict better. We repeat this process until our model has converged and can no longer learn. There are three key parameters to be chosen for this process (See [Table 2]()).\n",
    "\n",
    "* **Metric**: How to measure the performance of our model using a **metric**. We used [accuracy](https://developers.google.com/machine-learning/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=accuracy#accuracy) as the metric in our experiments.\n",
    "* **Loss function**: A function that is used to calculate a loss value that the training process then attempts to minimize by tuning the network weights. For classification problems, cross-entropy loss works well.\n",
    "* **Optimizer**: A function that decides how the network weights will be updated based on the output of the loss function. We used the popular [Adam](https://arxiv.org/abs/1412.6980) optimizer in our experiments.  \n",
    "\n",
    "In Keras, we can pass these learning parameters to a model using the [compile](https://keras.io/getting-started/sequential-model-guide/#compilation) method.\n",
    "\n",
    "<p align=\"left\">Learning parameter|\t<p style=\"text-align:center\">Value</p>\n",
    "---|---\n",
    "<p align=\"left\">Metric</p>\t|<p style=\"text-align:center\">accuracy</p>\n",
    "<p align=\"left\">Loss function - binary classification</p>\t|<p style=\"text-align:center\">binary_crossentropy</p>\n",
    "<p align=\"left\">Loss function - multi class classification</p>\t|<p style=\"text-align:center\">sparse_categorical_crossentropy</p>\n",
    "<p align=\"left\">Optimizer</p>\t|<p style=\"text-align:center\">adam</p>\n",
    "\n",
    "<p style=\"text-align:center\">**Table 2: Learning parameters**</p>\n",
    "\n",
    "The actual training happens using the [fit](https://keras.io/getting-started/sequential-model-guide/#training) method. Depending on the size of your dataset, this is the method in which most compute cycles will be spent. In each training iteration, batch_size number of samples from your training data are used to compute the loss, and the weights are updated once, based on this value. The training process completes an epoch once the model has seen the entire training dataset. At the end of each epoch, we use the validation dataset to evaluate how well the model is learning. We repeat training using the dataset for a predetermined number of epochs. We may optimize this by stopping early, when the validation accuracy stabilizes between consecutive epochs, showing that the model is not training anymore.\n",
    "\n",
    "<p align=\"left\">Training hyperparameter</p>\t|<p style=\"text-align:center\">Value</p>\n",
    "---|---\n",
    "<p align=\"left\">Learning rate</p>\t|<p style=\"text-align:center\">1e-3</p>\n",
    "<p align=\"left\">Epochs</p>\t|<p style=\"text-align:center\">1000</p>\n",
    "<p align=\"left\">Batch size</p>\t|<p style=\"text-align:center\">512</p>\n",
    "<p align=\"left\">Early stopping</p>\t|<p style=\"text-align:center\">parameter: val_loss, patience: 1</p>\n",
    "\n",
    "<p style=\"text-align:center\">**Table 3: Training hyperparameters**</p>\n",
    "\n",
    "The following Keras code implements the training process using the parameters chosen in the Tables 2 & 3 above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = explore_data.get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val = vectorize_data.ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = build_model.mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('IMDb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Tune Hyperparameters\n",
    "We had to choose a number of hyperparameters for defining and training the model. We relied on intuition, examples and best practice recommendations. Our first choice of hyperparameter values, however, may not yield the best results. It only gives us a good starting point for training. Every problem is different and tuning these hyperparameters will help refine our model to better represent the particularities of the problem at hand. Let’s take a look at some of the hyperparameters we used and what it means to tune them:\n",
    "* **Number of layers in the model**: The number of layers in a neural network is an indicator of its complexity. We must be careful in choosing this value. Too many layers will allow the model to learn too much information about the training data, causing overfitting. Too few layers can limit the model’s learning ability, causing underfitting. For text classification datasets, we experimented with one, two, and three-layer MLPs. Models with two layers performed well, and in some cases better than three-layer models. Similarly, we tried sepCNNs with four and six layers, and the four-layer models performed well.\n",
    "\n",
    "\n",
    "* **Number of units per layer**: The units in a layer must hold the information for the transformation that a layer performs. For the first layer, this is driven by the number of features. In subsequent layers, the number of units depends on the choice of expanding or contracting the representation from the previous layer. Try to minimize the information loss between layers. We tried unit values in the range [8, 16, 32, 64], and 32/64 units worked well.\n",
    "\n",
    "\n",
    "* **Dropout rate**: Dropout layers are used in the model for [regularization](https://developers.google.com/machine-learning/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=dropout-regularization#dropout_regularization). They define the fraction of input to drop as a precaution for overfitting. Recommended range: 0.2–0.5.\n",
    "\n",
    "\n",
    "* **Learning rate**: This is the rate at which the neural network weights change between iterations. A large learning rate may cause large swings in the weights, and we may never find their optimal values. A low learning rate is good, but the model will take more iterations to converge. It is a good idea to start low, say at 1e-4. If the training is very slow, increase this value. If your model is not learning, try decreasing learning rate.\n",
    "\n",
    "There are couple of additional hyperparameters we tuned that are specific to our sepCNN model:\n",
    "\n",
    "1. **Kernel size**: The size of the convolution window. Recommended values: 3 or 5.\n",
    "\n",
    "2. **Embedding dimensions**: The number of dimensions we want to use to represent word embeddings—i.e., the size of each word vector. Recommended values: 50–300. In our experiments, we used GloVe embeddings with 200 dimensions with a pre- trained embedding layer.\n",
    "\n",
    "Play around with these hyperparameters and see what works best. Once you have chosen the best-performing hyperparameters for your use case, your model is ready to be deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Deploy Your Model\n",
    "You can train, tune and deploy machine learning models on Google Cloud. See the following resources for guidance on deploying your model to production:\n",
    "\n",
    "* [Tutorial](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html#exporting-a-model-with-tensorflow-serving) on how to export a Keras model with TensorFlow serving.\n",
    "* TensorFlow serving [documentation](https://www.tensorflow.org/serving/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=tensorflow&utm_term=documentation).\n",
    "* [Guide](https://cloud.google.com/ml-engine/docs/how-tos?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=cloud&utm_term=guide) to training and deploying your model on Google Cloud.\n",
    "\n",
    "Please keep in mind the following key things when deploying your model:\n",
    "\n",
    "* Make sure your production data [follows the same distribution](https://developers.google.com/machine-learning/guides/rules-of-ml/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=rules-of-ml&utm_term=distribution#training-serving_skew) as your training and evaluation data.\n",
    "* Regularly re-evaluate by collecting more training data.\n",
    "* If your data distribution changes, retrain your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Text classification is a fundamental machine learning problem with applications across various products. In this guide, we have broken down the text classification workflow into several steps. For each step, we have suggested a customized approach based on the characteristics of your specific dataset. In particular, using the ratio of number of samples to the number of words per sample, we suggest a model type that gets you closer to the best performance quickly. The other steps are engineered around this choice. We hope that following the guide, the [accompanying code](https://github.com/google/eng-edu/tree/master/ml/guides/text_classification), and the [flowchart](https://developers.google.com/machine-learning/guides/text-classification/step-2-5#figure-5) will help you learn, understand, and get a swift first-cut solution to your text classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Batch Training\n",
    "Very large datasets may not fit in the memory allocated to your process. In the previous steps, we have set up a pipeline where we bring in the entire dataset in to the memory, prepare the data, and pass the working set to the training function. Instead, Keras provides an alternative training function ([fit_generator](https://keras.io/models/sequential/#fit_generator)) that pulls the data in batches. This allows us to apply the transformations in the data pipeline to only a small (a multiple of batch_size) part of the data. During our experiments, we used batching (code in GitHub) for datasets such as _DBPedia, Amazon reviews, Ag news, and Yelp reviews_.\n",
    "\n",
    "The following code illustrates how to generate data batches and feed them to [fit_generator](https://keras.io/models/sequential/#fit_generator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_generator(x, y, num_features, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "# Create training and validation generators.\n",
    "training_generator = _data_generator(\n",
    "    x_train, train_labels, num_features, batch_size)\n",
    "validation_generator = _data_generator(\n",
    "    x_val, val_labels, num_features, batch_size)\n",
    "\n",
    "# Get number of training steps. This indicated the number of steps it takes\n",
    "# to cover all samples in one epoch.\n",
    "steps_per_epoch = x_train.shape[0] // batch_size\n",
    "if x_train.shape[0] % batch_size:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# Get number of validation steps.\n",
    "validation_steps = x_val.shape[0] // batch_size\n",
    "if x_val.shape[0] % batch_size:\n",
    "    validation_steps += 1\n",
    "\n",
    "# Train and validate model.\n",
    "history = model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    epochs=epochs,\n",
    "    verbose=2)  # Logs once per epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
